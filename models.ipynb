{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PART 1 - LINEAR REGRESSION MODEL\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import numpy as np\n",
    "\n",
    "data = pd.read_csv('/new_weather_data.csv')\n",
    "\n",
    "# Selecting features\n",
    "features = ['Temperature..C.', 'Humidity', 'Wind.Speed..km.h.', 'Wind.Bearing..degrees.', 'Visibility..km.', 'Pressure..millibars.', 'Summary', 'Precip.Type']\n",
    "X = data[features]\n",
    "y = data['Apparent.Temperature..C.']\n",
    "\n",
    "\n",
    "numeric_features = ['Temperature..C.', 'Humidity', 'Wind.Speed..km.h.', 'Wind.Bearing..degrees.', 'Visibility..km.', 'Pressure..millibars.']\n",
    "categorical_features = ['Summary', 'Precip.Type']\n",
    "\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('scaler', StandardScaler())])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)])\n",
    "\n",
    "X_processed = preprocessor.fit_transform(X)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_processed, y, test_size=0.2, random_state=42)\n",
    "\n",
    "lr_model = LinearRegression()\n",
    "lr_model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluation\n",
    "y_pred_lr = lr_model.predict(X_test)\n",
    "lr_mse = mean_squared_error(y_test, y_pred_lr)\n",
    "lr_rmse = np.sqrt(lr_mse)\n",
    "lr_mae = mean_absolute_error(y_test, y_pred_lr)\n",
    "lr_r2 = r2_score(y_test, y_pred_lr)\n",
    "\n",
    "print(f'Linear Regression - R2: {lr_r2}, MSE: {lr_mse}, RMSE: {lr_rmse}, MAE: {lr_mae}')\n",
    "\n",
    "# Get feature importance\n",
    "feature_importance = best_gbr.feature_importances_\n",
    "\n",
    "encoded_cat_columns = preprocessor.named_transformers_['cat'].named_steps['onehot'].get_feature_names_out(input_features=categorical_features)\n",
    "all_feature_names = features_to_use + list(encoded_cat_columns)\n",
    "\n",
    "feature_importance_df = pd.DataFrame({'Feature': all_feature_names, 'Importance': feature_importance})\n",
    "feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "print(\"\\nFeature Importance:\")\n",
    "print(feature_importance_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PART 2 - LINEAR REGRESSION MODEL INTERPRETATION\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import pearsonr\n",
    "from sklearn.model_selection import learning_curve\n",
    "\n",
    "#part 2 b - Calculate the average of the target variable\n",
    "y_avg = np.mean(y_test)\n",
    "rmse_percentage = (lr_rmse / y_avg) * 100\n",
    "print(f\"RMSE as a percentage of the target variable's average: {rmse_percentage}%\")\n",
    "\n",
    "#part 2 - c\n",
    "# Scatter plot of actual vs predicted values\n",
    "plt.scatter(y_test, y_pred_lr, alpha=0.5)\n",
    "plt.xlabel('Actual')\n",
    "plt.ylabel('Predicted')\n",
    "plt.title('Actual vs Predicted Values')\n",
    "plt.plot(np.unique(y_test), np.poly1d(np.polyfit(y_test, y_pred_lr, 1))(np.unique(y_test)), color='red')  # Linear fit\n",
    "plt.show()\n",
    "\n",
    "#part 2 d - Calculate Pearson's correlation\n",
    "correlation, _ = pearsonr(y_test, y_pred_lr)\n",
    "print(f\"Pearson's correlation: {correlation}\")\n",
    "\n",
    "#part 2 e - Function to plot learning curves\n",
    "def plot_learning_curves(model, X, y):\n",
    "    train_sizes, train_scores, validation_scores = learning_curve(model, X, y, train_sizes=np.linspace(0.1, 1.0, 10), cv=3, scoring='neg_mean_squared_error')\n",
    "\n",
    "    train_scores_mean = -train_scores.mean(axis=1)\n",
    "    train_scores_std = train_scores.std(axis=1)\n",
    "    validation_scores_mean = -validation_scores.mean(axis=1)\n",
    "    validation_scores_std = validation_scores.std(axis=1)\n",
    "\n",
    "    plt.fill_between(train_sizes, train_scores_mean - train_scores_std, train_scores_mean + train_scores_std, color=\"r\", alpha=0.1)\n",
    "    plt.fill_between(train_sizes, validation_scores_mean - validation_scores_std, validation_scores_mean + validation_scores_std, color=\"g\", alpha=0.1)\n",
    "\n",
    "    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\", label=\"Training score\")\n",
    "    plt.plot(train_sizes, validation_scores_mean, 'o-', color=\"g\", label=\"Cross-validation score\")\n",
    "\n",
    "    plt.title('Learning curves')\n",
    "    plt.xlabel('Training examples')\n",
    "    plt.ylabel('MSE')\n",
    "    plt.legend(loc=\"best\")\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "\n",
    "plot_learning_curves(lr_model, X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PART 1 - NEURAL NETWORKS MODEL\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "import numpy as np\n",
    "\n",
    "data = pd.read_csv('/new_weather_data.csv')\n",
    "\n",
    "features = ['Temperature..C.', 'Humidity', 'Wind.Speed..km.h.', 'Wind.Bearing..degrees.', 'Visibility..km.', 'Pressure..millibars.', 'Summary', 'Precip.Type']\n",
    "X = data[features]\n",
    "y = data['Apparent.Temperature..C.']\n",
    "\n",
    "numeric_features = ['Temperature..C.', 'Humidity', 'Wind.Speed..km.h.', 'Wind.Bearing..degrees.', 'Visibility..km.', 'Pressure..millibars.']\n",
    "categorical_features = ['Summary', 'Precip.Type']\n",
    "\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('scaler', StandardScaler())])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)])\n",
    "\n",
    "X_processed = preprocessor.fit_transform(X)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_processed, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Hyperparameters\n",
    "learning_rate = 0.001\n",
    "n_epochs = 100\n",
    "batch_size = 16\n",
    "\n",
    "# Neural Network Model\n",
    "model = Sequential([\n",
    "    Dense(256, activation='relu', input_shape=(X_train.shape[1],)),\n",
    "    Dropout(0.3),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dense(1)\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='mean_squared_error', metrics=['mae'])\n",
    "history = model.fit(X_train, y_train, epochs=n_epochs, batch_size=batch_size, validation_split=0.2)\n",
    "\n",
    "# Evaluation\n",
    "y_pred = model.predict(X_test)\n",
    "nn_mse = mean_squared_error(y_test, y_pred)\n",
    "nn_rmse = np.sqrt(nn_mse)\n",
    "nn_mae = mean_absolute_error(y_test, y_pred)\n",
    "nn_r2 = r2_score(y_test, y_pred)\n",
    "print(f'Neural Network - MSE: {nn_mse}, RMSE: {nn_rmse}, MAE: {nn_mae}, R^2: {nn_r2}, Learning Rate: {learning_rate}, Epochs: {n_epochs}, Batch Size: {batch_size}')\n",
    "\n",
    "# Get feature importance\n",
    "feature_importance = best_gbr.feature_importances_\n",
    "\n",
    "encoded_cat_columns = preprocessor.named_transformers_['cat'].named_steps['onehot'].get_feature_names_out(input_features=categorical_features)\n",
    "all_feature_names = features_to_use + list(encoded_cat_columns)\n",
    "\n",
    "feature_importance_df = pd.DataFrame({'Feature': all_feature_names, 'Importance': feature_importance})\n",
    "feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "print(\"\\nFeature Importance:\")\n",
    "print(feature_importance_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PART 2 - NEURAL NETWORK MODEL INTERPRETATION\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "# Part b - Calculate the percentage of RMSE to the average of the target variable\n",
    "y_avg = np.mean(y_test)\n",
    "rmse_percentage = (nn_rmse / y_avg) * 100\n",
    "print(f\"RMSE as a percentage of the target variable's average: {rmse_percentage:.2f}%\")\n",
    "\n",
    "# Part c - Scatter plot of actual vs predicted values\n",
    "plt.scatter(y_test, y_pred.flatten(), alpha=0.5)\n",
    "plt.plot(np.unique(y_test), np.poly1d(np.polyfit(y_test, y_pred.flatten(), 1))(np.unique(y_test)), color='red')\n",
    "plt.xlabel('Actual')\n",
    "plt.ylabel('Predicted')\n",
    "plt.title('Actual vs Predicted Values')\n",
    "plt.show()\n",
    "\n",
    "# Part d - Calculate Pearson's correlation\n",
    "correlation, _ = pearsonr(y_test, y_pred.flatten())\n",
    "print(f\"Pearson's correlation: {correlation:.2f}\")\n",
    "\n",
    "# Part e - Plot the learning curves using the history from model training\n",
    "def plot_learning_curves(history):\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(history.history['loss'], label='Training Loss')\n",
    "    plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "    plt.title('Learning Curves')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "plot_learning_curves(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PART 1 - DESICION TREE REGRESSION MODEL\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "import numpy as np\n",
    "\n",
    "data = pd.read_csv('/new_weather_data.csv')\n",
    "\n",
    "features_to_use = ['Temperature..C.', 'Humidity', 'Wind.Speed..km.h.', 'Wind.Bearing..degrees.', 'Visibility..km.', 'Pressure..millibars.']\n",
    "categorical_features = ['Summary', 'Precip.Type']  # Assuming these are the only categorical features\n",
    "\n",
    "X = data[features_to_use + categorical_features]\n",
    "y = data['Apparent.Temperature..C.']\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('cat', categorical_transformer, categorical_features)],\n",
    "    remainder='passthrough')\n",
    "\n",
    "X_processed = preprocessor.fit_transform(X)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_processed, y, test_size=0.2, random_state=42)\n",
    "\n",
    "param_grid = {\n",
    "    'max_depth': [None, 10, 20, 30],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "dt = DecisionTreeRegressor(random_state=42)\n",
    "grid_search = GridSearchCV(estimator=dt, param_grid=param_grid, cv=3, n_jobs=-1, verbose=2)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Best hyperparameters\n",
    "best_params = grid_search.best_params_\n",
    "best_dt = grid_search.best_estimator_\n",
    "y_pred_dt = best_dt.predict(X_test)\n",
    "\n",
    "# Evaluating the model\n",
    "dt_r2 = r2_score(y_test, y_pred_dt)\n",
    "dt_rmse = np.sqrt(mean_squared_error(y_test, y_pred_dt))\n",
    "dt_mse = mean_squared_error(y_test, y_pred_dt)\n",
    "dt_mae = mean_absolute_error(y_test, y_pred_dt)\n",
    "\n",
    "best_dt.fit(X_processed, y)\n",
    "\n",
    "print(f'Decision Tree - R2: {dt_r2}, RMSE: {dt_rmse}, MSE: {dt_mse}, MAE: {dt_mae}')\n",
    "print(f'Best Hyperparameters: {best_params}')\n",
    "\n",
    "# Get feature importance\n",
    "feature_importance = best_gbr.feature_importances_\n",
    "\n",
    "encoded_cat_columns = preprocessor.named_transformers_['cat'].named_steps['onehot'].get_feature_names_out(input_features=categorical_features)\n",
    "all_feature_names = features_to_use + list(encoded_cat_columns)\n",
    "\n",
    "feature_importance_df = pd.DataFrame({'Feature': all_feature_names, 'Importance': feature_importance})\n",
    "feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "print(\"\\nFeature Importance:\")\n",
    "print(feature_importance_df)\n",
    "\n",
    "# Function to recursively traverse the tree and extract rules\n",
    "def tree_to_rules(tree, feature_names):\n",
    "    tree_ = tree.tree_\n",
    "    feature_name = [\n",
    "        feature_names[i] if i != _tree.TREE_UNDEFINED else \"undefined!\"\n",
    "        for i in tree_.feature\n",
    "    ]\n",
    "\n",
    "    def recurse(node):\n",
    "        if tree_.feature[node] != _tree.TREE_UNDEFINED:\n",
    "            name = feature_name[node]\n",
    "            threshold = tree_.threshold[node]\n",
    "            print(f\"If {name} <= {threshold}:\")\n",
    "            recurse(tree_.children_left[node])\n",
    "\n",
    "            print(f\"Else (If {name} > {threshold}):\")\n",
    "            recurse(tree_.children_right[node])\n",
    "        else:\n",
    "            print(f\"Predict {tree_.value[node]}\")\n",
    "\n",
    "    recurse(0)\n",
    "\n",
    "tree_to_rules(best_dt, all_feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PART 2 - DESICION TREE REGRESSION MODEL INTERPRETATION\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "# Part b - Calculate the percentage of RMSE to the average of the target variable\n",
    "y_avg_dt = np.mean(y_test)\n",
    "rmse_percentage_dt = (dt_rmse / y_avg_dt) * 100\n",
    "print(f\"RMSE as a percentage of the target variable's average: {rmse_percentage_dt:.2f}%\")\n",
    "\n",
    "# Part c - Scatter plot of actual vs predicted values for Decision Tree\n",
    "plt.scatter(y_test, y_pred_dt, alpha=0.5)\n",
    "plt.xlabel('Actual')\n",
    "plt.ylabel('Predicted')\n",
    "plt.title('Actual vs Predicted Values - Decision Tree')\n",
    "plt.plot(np.unique(y_test), np.poly1d(np.polyfit(y_test, y_pred_dt, 1))(np.unique(y_test)), color='red')\n",
    "plt.show()\n",
    "\n",
    "# Part d - Calculate Pearson's correlation for Decision Tree\n",
    "correlation_dt, _ = pearsonr(y_test, y_pred_dt)\n",
    "print(f\"Pearson's correlation for Decision Tree: {correlation_dt:.2f}\")\n",
    "\n",
    "# Part e - Learning curves are typically used with models that can be incrementally trained.\n",
    "# Decision trees do not fit this category as they do not support partial fitting.\n",
    "# However, we can still generate a plot to show model complexity vs error.\n",
    "# This would involve varying the 'max_depth' of the tree and plotting the training and validation error.\n",
    "\n",
    "# Function to calculate error metrics for varying model complexities (max_depth)\n",
    "def model_complexity_curve(X_train, y_train, X_test, y_test, max_depths):\n",
    "    train_errors = []\n",
    "    test_errors = []\n",
    "    for max_depth in max_depths:\n",
    "        model = DecisionTreeRegressor(max_depth=max_depth)\n",
    "        model.fit(X_train, y_train)\n",
    "        train_predictions = model.predict(X_train)\n",
    "        test_predictions = model.predict(X_test)\n",
    "        train_errors.append(mean_squared_error(y_train, train_predictions))\n",
    "        test_errors.append(mean_squared_error(y_test, test_predictions))\n",
    "\n",
    "    return train_errors, test_errors\n",
    "\n",
    "max_depths = np.arange(1, 21)\n",
    "train_errors, test_errors = model_complexity_curve(X_train, y_train, X_test, y_test, max_depths)\n",
    "\n",
    "plt.plot(max_depths, train_errors, label='Training Error')\n",
    "plt.plot(max_depths, test_errors, label='Validation Error')\n",
    "plt.xlabel('Max Depth of Decision Tree')\n",
    "plt.ylabel('Mean Squared Error')\n",
    "plt.title('Model Complexity vs Error')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PART 1 - GRADIENT BOOSTING REGRESSOR MODEL\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "import numpy as np\n",
    "\n",
    "data = pd.read_csv('/new_weather_data.csv')\n",
    "\n",
    "features_to_use = ['Temperature..C.', 'Humidity', 'Wind.Speed..km.h.', 'Wind.Bearing..degrees.', 'Visibility..km.', 'Pressure..millibars.']\n",
    "categorical_features = ['Summary', 'Precip.Type']\n",
    "\n",
    "X = data[features_to_use + categorical_features]\n",
    "y = data['Apparent.Temperature..C.']\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('cat', categorical_transformer, categorical_features)],\n",
    "    remainder='passthrough')\n",
    "\n",
    "X_processed = preprocessor.fit_transform(X)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_processed, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Best hyperparameters\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200],\n",
    "    'max_depth': [3, 4, 5],\n",
    "    'learning_rate': [0.01, 0.1]\n",
    "}\n",
    "\n",
    "gbr = GradientBoostingRegressor(random_state=42)\n",
    "grid_search = GridSearchCV(estimator=gbr, param_grid=param_grid, cv=3, n_jobs=-1, verbose=2)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "best_params = grid_search.best_params_\n",
    "\n",
    "best_gbr = grid_search.best_estimator_\n",
    "\n",
    "y_pred = best_gbr.predict(X_test)\n",
    "\n",
    "# Evaluating the model\n",
    "gbr_r2 = r2_score(y_test, y_pred)\n",
    "gbr_rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "gbr_mse = mean_squared_error(y_test, y_pred)\n",
    "gbr_mae = mean_absolute_error(y_test, y_pred)\n",
    "\n",
    "print(f'Best Hyperparameters: {best_params}')\n",
    "print(f'Gradient Boosted Regressor - R2: {gbr_r2}, RMSE: {gbr_rmse}, MSE: {gbr_mse}, MAE: {gbr_mae}')\n",
    "\n",
    "# Get feature importance\n",
    "feature_importance = best_gbr.feature_importances_\n",
    "\n",
    "encoded_cat_columns = preprocessor.named_transformers_['cat'].named_steps['onehot'].get_feature_names_out(input_features=categorical_features)\n",
    "all_feature_names = features_to_use + list(encoded_cat_columns)\n",
    "\n",
    "feature_importance_df = pd.DataFrame({'Feature': all_feature_names, 'Importance': feature_importance})\n",
    "feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "print(\"\\nFeature Importance:\")\n",
    "print(feature_importance_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PART 2 - Gradient Boosting Regressor Model Interpretation\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import pearsonr\n",
    "from sklearn.model_selection import learning_curve\n",
    "\n",
    "# Part c - Scatter plot of actual vs predicted values for Gradient Boosting Regressor\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(y_test, y_pred, alpha=0.5)\n",
    "plt.xlabel('Actual')\n",
    "plt.ylabel('Predicted')\n",
    "plt.title('Actual vs Predicted Values - Gradient Boosting Regressor')\n",
    "m, b = np.polyfit(y_test, y_pred, 1)\n",
    "plt.plot(y_test, m*y_test + b, color='red')\n",
    "plt.show()\n",
    "\n",
    "# Part d - Calculate Pearson's correlation\n",
    "correlation_gbr, _ = pearsonr(y_test, y_pred)\n",
    "print(f\"Pearson's correlation for Gradient Boosting Regressor: {correlation_gbr:.2f}\")\n",
    "\n",
    "# Part e - Learning curves\n",
    "def plot_learning_curves(estimator, X, y):\n",
    "    train_sizes, train_scores, test_scores = learning_curve(\n",
    "        estimator, X, y, train_sizes=np.linspace(0.1, 1.0, 10), cv=3, n_jobs=-1, scoring='neg_mean_squared_error')\n",
    "\n",
    "    train_scores_mean = -np.mean(train_scores, axis=1)\n",
    "    train_scores_std = np.std(train_scores, axis=1)\n",
    "    test_scores_mean = -np.mean(test_scores, axis=1)\n",
    "    test_scores_std = np.std(test_scores, axis=1)\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n",
    "                     train_scores_mean + train_scores_std, alpha=0.1, color=\"r\")\n",
    "    plt.fill_between(train_sizes, test_scores_mean - test_scores_std,\n",
    "                     test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n",
    "    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\", label=\"Training score\")\n",
    "    plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\", label=\"Cross-validation score\")\n",
    "\n",
    "    plt.title('Learning Curves')\n",
    "    plt.xlabel('Training examples')\n",
    "    plt.ylabel('Negative Mean Squared Error')\n",
    "    plt.legend(loc=\"best\")\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "\n",
    "plot_learning_curves(best_gbr, X_train, y_train)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
